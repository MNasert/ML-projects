{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3797da65ef10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[1;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m         raise RuntimeError(\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_capability(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_properties(0))\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=10, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], requires_grad=True)\n",
      "Sigmoid()\n",
      "Linear(in_features=10, out_features=1, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "epoch:  1 Loss:  31.21257781982422\n",
      "epoch:  2 Loss:  9.828660011291504\n",
      "epoch:  3 Loss:  2.1334309577941895\n",
      "epoch:  4 Loss:  0.9510778188705444\n",
      "epoch:  5 Loss:  0.6984660029411316\n",
      "epoch:  6 Loss:  0.6390078067779541\n",
      "epoch:  7 Loss:  0.6100386381149292\n",
      "epoch:  8 Loss:  0.5868361592292786\n",
      "epoch:  9 Loss:  0.5651189088821411\n",
      "epoch:  10 Loss:  0.5441954731941223\n",
      "epoch:  11 Loss:  0.5239441990852356\n",
      "epoch:  12 Loss:  0.5043361783027649\n",
      "epoch:  13 Loss:  0.48535823822021484\n",
      "epoch:  14 Loss:  0.46699798107147217\n",
      "epoch:  15 Loss:  0.4492436647415161\n",
      "epoch:  16 Loss:  0.43208402395248413\n",
      "epoch:  17 Loss:  0.4155070185661316\n",
      "epoch:  18 Loss:  0.3995008170604706\n",
      "epoch:  19 Loss:  0.3840527832508087\n",
      "epoch:  20 Loss:  0.3691505789756775\n",
      "epoch:  21 Loss:  0.35478198528289795\n",
      "epoch:  22 Loss:  0.34093353152275085\n",
      "epoch:  23 Loss:  0.32759326696395874\n",
      "epoch:  24 Loss:  0.3147481083869934\n",
      "epoch:  25 Loss:  0.3023848235607147\n",
      "epoch:  26 Loss:  0.29049059748649597\n",
      "epoch:  27 Loss:  0.2790524661540985\n",
      "epoch:  28 Loss:  0.26805710792541504\n",
      "epoch:  29 Loss:  0.2574918568134308\n",
      "epoch:  30 Loss:  0.2473435401916504\n",
      "epoch:  31 Loss:  0.23759903013706207\n",
      "epoch:  32 Loss:  0.22824633121490479\n",
      "epoch:  33 Loss:  0.2192719429731369\n",
      "epoch:  34 Loss:  0.21066391468048096\n",
      "epoch:  35 Loss:  0.20240961015224457\n",
      "epoch:  36 Loss:  0.19449684023857117\n",
      "epoch:  37 Loss:  0.18691356480121613\n",
      "epoch:  38 Loss:  0.1796479970216751\n",
      "epoch:  39 Loss:  0.17268872261047363\n",
      "epoch:  40 Loss:  0.16602447628974915\n",
      "epoch:  41 Loss:  0.15964385867118835\n",
      "epoch:  42 Loss:  0.15353623032569885\n",
      "epoch:  43 Loss:  0.1476905345916748\n",
      "epoch:  44 Loss:  0.1420968472957611\n",
      "epoch:  45 Loss:  0.13674518465995789\n",
      "epoch:  46 Loss:  0.13162577152252197\n",
      "epoch:  47 Loss:  0.12672877311706543\n",
      "epoch:  48 Loss:  0.12204532325267792\n",
      "epoch:  49 Loss:  0.11756635457277298\n",
      "epoch:  50 Loss:  0.1132831871509552\n",
      "epoch:  51 Loss:  0.10918755829334259\n",
      "epoch:  52 Loss:  0.10527122765779495\n",
      "epoch:  53 Loss:  0.10152672976255417\n",
      "epoch:  54 Loss:  0.09794613718986511\n",
      "epoch:  55 Loss:  0.09452242404222488\n",
      "epoch:  56 Loss:  0.09124848991632462\n",
      "epoch:  57 Loss:  0.08811803162097931\n",
      "epoch:  58 Loss:  0.08512409776449203\n",
      "epoch:  59 Loss:  0.08226054906845093\n",
      "epoch:  60 Loss:  0.07952164858579636\n",
      "epoch:  61 Loss:  0.07690171897411346\n",
      "epoch:  62 Loss:  0.07439520210027695\n",
      "epoch:  63 Loss:  0.07199668139219284\n",
      "epoch:  64 Loss:  0.06970137357711792\n",
      "epoch:  65 Loss:  0.06750427931547165\n",
      "epoch:  66 Loss:  0.0654011070728302\n",
      "epoch:  67 Loss:  0.06338715553283691\n",
      "epoch:  68 Loss:  0.061458393931388855\n",
      "epoch:  69 Loss:  0.059610702097415924\n",
      "epoch:  70 Loss:  0.05784038454294205\n",
      "epoch:  71 Loss:  0.05614394694566727\n",
      "epoch:  72 Loss:  0.0545174814760685\n",
      "epoch:  73 Loss:  0.0529579259455204\n",
      "epoch:  74 Loss:  0.05146213248372078\n",
      "epoch:  75 Loss:  0.05002713203430176\n",
      "epoch:  76 Loss:  0.04864991083741188\n",
      "epoch:  77 Loss:  0.04732801020145416\n",
      "epoch:  78 Loss:  0.0460585281252861\n",
      "epoch:  79 Loss:  0.044839046895504\n",
      "epoch:  80 Loss:  0.04366754740476608\n",
      "epoch:  81 Loss:  0.042541444301605225\n",
      "epoch:  82 Loss:  0.04145865514874458\n",
      "epoch:  83 Loss:  0.040417175740003586\n",
      "epoch:  84 Loss:  0.039415352046489716\n",
      "epoch:  85 Loss:  0.03845079615712166\n",
      "epoch:  86 Loss:  0.037522390484809875\n",
      "epoch:  87 Loss:  0.036628324538469315\n",
      "epoch:  88 Loss:  0.035766854882240295\n",
      "epoch:  89 Loss:  0.034936513751745224\n",
      "epoch:  90 Loss:  0.03413600102066994\n",
      "epoch:  91 Loss:  0.033364005386829376\n",
      "epoch:  92 Loss:  0.03261904418468475\n",
      "epoch:  93 Loss:  0.031900182366371155\n",
      "epoch:  94 Loss:  0.031206119805574417\n",
      "epoch:  95 Loss:  0.030535804107785225\n",
      "epoch:  96 Loss:  0.02988819219172001\n",
      "epoch:  97 Loss:  0.029262255877256393\n",
      "epoch:  98 Loss:  0.028657088056206703\n",
      "epoch:  99 Loss:  0.0280718095600605\n",
      "epoch:  100 Loss:  0.027505574747920036\n",
      "epoch:  101 Loss:  0.026957673951983452\n",
      "epoch:  102 Loss:  0.0264271330088377\n",
      "epoch:  103 Loss:  0.025913501158356667\n",
      "epoch:  104 Loss:  0.025415809825062752\n",
      "epoch:  105 Loss:  0.024933533743023872\n",
      "epoch:  106 Loss:  0.024466099217534065\n",
      "epoch:  107 Loss:  0.024012818932533264\n",
      "epoch:  108 Loss:  0.02357320673763752\n",
      "epoch:  109 Loss:  0.02314668335020542\n",
      "epoch:  110 Loss:  0.022732749581336975\n",
      "epoch:  111 Loss:  0.022330863401293755\n",
      "epoch:  112 Loss:  0.021940600126981735\n",
      "epoch:  113 Loss:  0.021561626344919205\n",
      "epoch:  114 Loss:  0.021193258464336395\n",
      "epoch:  115 Loss:  0.020835280418395996\n",
      "epoch:  116 Loss:  0.020487377420067787\n",
      "epoch:  117 Loss:  0.02014900930225849\n",
      "epoch:  118 Loss:  0.019819946959614754\n",
      "epoch:  119 Loss:  0.019499778747558594\n",
      "epoch:  120 Loss:  0.019188184291124344\n",
      "epoch:  121 Loss:  0.018884917721152306\n",
      "epoch:  122 Loss:  0.018589606508612633\n",
      "epoch:  123 Loss:  0.018302109092473984\n",
      "epoch:  124 Loss:  0.018022025004029274\n",
      "epoch:  125 Loss:  0.017749225720763206\n",
      "epoch:  126 Loss:  0.017483266070485115\n",
      "epoch:  127 Loss:  0.017224090173840523\n",
      "epoch:  128 Loss:  0.01697140373289585\n",
      "epoch:  129 Loss:  0.016725054010748863\n",
      "epoch:  130 Loss:  0.016484679654240608\n",
      "epoch:  131 Loss:  0.016250334680080414\n",
      "epoch:  132 Loss:  0.016021622344851494\n",
      "epoch:  133 Loss:  0.015798388049006462\n",
      "epoch:  134 Loss:  0.015580535866320133\n",
      "epoch:  135 Loss:  0.015367870219051838\n",
      "epoch:  136 Loss:  0.015160243026912212\n",
      "epoch:  137 Loss:  0.014957420527935028\n",
      "epoch:  138 Loss:  0.014759308658540249\n",
      "epoch:  139 Loss:  0.014565812423825264\n",
      "epoch:  140 Loss:  0.014376729726791382\n",
      "epoch:  141 Loss:  0.014191924594342709\n",
      "epoch:  142 Loss:  0.014011314138770103\n",
      "epoch:  143 Loss:  0.013834725134074688\n",
      "epoch:  144 Loss:  0.013662111945450306\n",
      "epoch:  145 Loss:  0.013493291102349758\n",
      "epoch:  146 Loss:  0.01332817506045103\n",
      "epoch:  147 Loss:  0.013166670687496662\n",
      "epoch:  148 Loss:  0.01300865225493908\n",
      "epoch:  149 Loss:  0.012854030355811119\n",
      "epoch:  150 Loss:  0.012702819891273975\n",
      "epoch:  151 Loss:  0.012554711662232876\n",
      "epoch:  152 Loss:  0.012409772723913193\n",
      "epoch:  153 Loss:  0.012267913669347763\n",
      "epoch:  154 Loss:  0.012128930538892746\n",
      "epoch:  155 Loss:  0.011992866173386574\n",
      "epoch:  156 Loss:  0.011859557591378689\n",
      "epoch:  157 Loss:  0.01172911562025547\n",
      "epoch:  158 Loss:  0.011601187288761139\n",
      "epoch:  159 Loss:  0.011475883424282074\n",
      "epoch:  160 Loss:  0.011353176087141037\n",
      "epoch:  161 Loss:  0.011232846416532993\n",
      "epoch:  162 Loss:  0.011114980094134808\n",
      "epoch:  163 Loss:  0.010999390855431557\n",
      "epoch:  164 Loss:  0.010886085219681263\n",
      "epoch:  165 Loss:  0.010775038041174412\n",
      "epoch:  166 Loss:  0.01066608913242817\n",
      "epoch:  167 Loss:  0.01055927388370037\n",
      "epoch:  168 Loss:  0.010454499162733555\n",
      "epoch:  169 Loss:  0.010351736098527908\n",
      "epoch:  170 Loss:  0.010250912047922611\n",
      "epoch:  171 Loss:  0.010152016766369343\n",
      "epoch:  172 Loss:  0.010054980404675007\n",
      "epoch:  173 Loss:  0.009959770366549492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0264], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################  Erstes handmade NN  ####################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "n_in =10#input\n",
    "n_h=5 #hidden layer size\n",
    "n_out=1#output\n",
    "batch_size=10#batch size\n",
    "y = torch.tensor([[0.],[1.],[2.],[3.],[4.],[5.]])\n",
    "x=torch.tensor([[0.],[1.],[2.],[3.],[4.],[5.]])\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(1.0)\n",
    "        print(m.weight)\n",
    "net =nn.Sequential(nn.Linear(1,10),\n",
    "                   nn.Sigmoid(),\n",
    "                   nn.Linear(10, 1),\n",
    "                   )\n",
    "net.apply(init_weights)\n",
    "            \n",
    "criterion = torch.nn.MSELoss()#wie wird loss berechnet            \n",
    "loss=criterion(y[1],x[0])       \n",
    "\n",
    "epoch=0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.1)\n",
    "\n",
    "\n",
    "while loss.item()>0.01:\n",
    "    epoch+=1\n",
    "    y_pred = net(x)#model.predict --> ist das selbe in sklearn\n",
    "\n",
    "    # loss wird anhand von criterion berechnet\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"epoch: \",epoch,\"Loss: \",loss.item())\n",
    "\n",
    "    # Gradienten auf null setzen\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backwardpass aller Gewichte und Biases anhand Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameterupdate\n",
    "    optimizer.step()\n",
    "net(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_nr:  0\n",
      "loss:  23.805728912353516\n",
      "batch_nr:  1\n",
      "loss:  393537.09375\n",
      "batch_nr:  2\n",
      "loss:  20.089189529418945\n",
      "batch_nr:  3\n",
      "loss:  20.46936798095703\n",
      "batch_nr:  4\n",
      "loss:  28.877511978149414\n",
      "batch_nr:  5\n",
      "loss:  23.355735778808594\n",
      "batch_nr:  6\n",
      "loss:  18.843534469604492\n",
      "batch_nr:  7\n",
      "loss:  18.572628021240234\n",
      "batch_nr:  8\n",
      "loss:  20.642536163330078\n",
      "batch_nr:  9\n",
      "loss:  21.895631790161133\n",
      "batch_nr:  10\n",
      "loss:  30.335912704467773\n",
      "batch_nr:  11\n",
      "loss:  22.171131134033203\n",
      "batch_nr:  12\n",
      "loss:  20.118589401245117\n",
      "batch_nr:  13\n",
      "loss:  25.593122482299805\n",
      "batch_nr:  14\n",
      "loss:  14.468789100646973\n",
      "batch_nr:  15\n",
      "loss:  13.650882720947266\n",
      "batch_nr:  16\n",
      "loss:  27.748071670532227\n",
      "batch_nr:  17\n",
      "loss:  15.77002239227295\n",
      "batch_nr:  18\n",
      "loss:  18.963348388671875\n",
      "batch_nr:  19\n",
      "loss:  14.75402545928955\n",
      "batch_nr:  20\n",
      "loss:  18.622663497924805\n",
      "batch_nr:  21\n",
      "loss:  28.25505256652832\n",
      "batch_nr:  22\n",
      "loss:  20.581689834594727\n",
      "batch_nr:  23\n",
      "loss:  12.7889404296875\n",
      "batch_nr:  24\n",
      "loss:  29.435531616210938\n",
      "batch_nr:  25\n",
      "loss:  31.663965225219727\n",
      "batch_nr:  26\n",
      "loss:  37.8682975769043\n",
      "batch_nr:  27\n",
      "loss:  17.36338996887207\n",
      "batch_nr:  28\n",
      "loss:  17.713685989379883\n",
      "batch_nr:  29\n",
      "loss:  23.7086124420166\n",
      "batch_nr:  30\n",
      "loss:  8.144218444824219\n",
      "batch_nr:  31\n",
      "loss:  12.429574012756348\n",
      "batch_nr:  32\n",
      "loss:  10.058940887451172\n",
      "batch_nr:  33\n",
      "loss:  22.50214958190918\n",
      "batch_nr:  34\n",
      "loss:  16.507957458496094\n",
      "batch_nr:  35\n",
      "loss:  27.657865524291992\n",
      "batch_nr:  36\n",
      "loss:  12.28074836730957\n",
      "batch_nr:  37\n",
      "loss:  25.545839309692383\n",
      "batch_nr:  38\n",
      "loss:  12.447466850280762\n",
      "batch_nr:  39\n",
      "loss:  16.94866943359375\n",
      "batch_nr:  40\n",
      "loss:  21.363445281982422\n",
      "batch_nr:  41\n",
      "loss:  17.631677627563477\n",
      "batch_nr:  42\n",
      "loss:  10.916236877441406\n",
      "batch_nr:  43\n",
      "loss:  22.591398239135742\n",
      "batch_nr:  44\n",
      "loss:  28.55131721496582\n",
      "batch_nr:  45\n",
      "loss:  24.836637496948242\n",
      "batch_nr:  46\n",
      "loss:  22.74724769592285\n",
      "batch_nr:  47\n",
      "loss:  10.404004096984863\n",
      "batch_nr:  48\n",
      "loss:  11.521995544433594\n",
      "batch_nr:  49\n",
      "loss:  30.51677703857422\n",
      "batch_nr:  50\n",
      "loss:  20.187124252319336\n",
      "batch_nr:  51\n",
      "loss:  18.127655029296875\n",
      "batch_nr:  52\n",
      "loss:  12.756537437438965\n",
      "batch_nr:  53\n",
      "loss:  15.525076866149902\n",
      "batch_nr:  54\n",
      "loss:  20.870845794677734\n",
      "batch_nr:  55\n",
      "loss:  22.342174530029297\n",
      "batch_nr:  56\n",
      "loss:  31.120834350585938\n",
      "batch_nr:  57\n",
      "loss:  17.762666702270508\n",
      "batch_nr:  58\n",
      "loss:  26.536672592163086\n",
      "batch_nr:  59\n",
      "loss:  10.447154998779297\n",
      "batch_nr:  60\n",
      "loss:  22.2235164642334\n",
      "batch_nr:  61\n",
      "loss:  22.806589126586914\n",
      "batch_nr:  62\n",
      "loss:  22.181957244873047\n",
      "batch_nr:  63\n",
      "loss:  6.67622709274292\n",
      "batch_nr:  64\n",
      "loss:  16.969223022460938\n",
      "batch_nr:  65\n",
      "loss:  28.129762649536133\n",
      "batch_nr:  66\n",
      "loss:  10.608424186706543\n",
      "batch_nr:  67\n",
      "loss:  27.30231285095215\n",
      "batch_nr:  68\n",
      "loss:  39.24930191040039\n",
      "batch_nr:  69\n",
      "loss:  26.591886520385742\n",
      "batch_nr:  70\n",
      "loss:  20.931766510009766\n",
      "batch_nr:  71\n",
      "loss:  16.569103240966797\n",
      "batch_nr:  72\n",
      "loss:  18.752838134765625\n",
      "batch_nr:  73\n",
      "loss:  23.695283889770508\n",
      "batch_nr:  74\n",
      "loss:  26.568065643310547\n",
      "batch_nr:  75\n",
      "loss:  21.373085021972656\n",
      "batch_nr:  76\n",
      "loss:  27.706745147705078\n",
      "batch_nr:  77\n",
      "loss:  13.555062294006348\n",
      "batch_nr:  78\n",
      "loss:  22.443906784057617\n",
      "batch_nr:  79\n",
      "loss:  21.99546241760254\n",
      "batch_nr:  80\n",
      "loss:  15.000470161437988\n",
      "batch_nr:  81\n",
      "loss:  14.44625473022461\n",
      "batch_nr:  82\n",
      "loss:  24.19607925415039\n",
      "batch_nr:  83\n",
      "loss:  15.269899368286133\n",
      "batch_nr:  84\n",
      "loss:  12.989316940307617\n",
      "batch_nr:  85\n",
      "loss:  22.58991050720215\n",
      "batch_nr:  86\n",
      "loss:  23.293476104736328\n",
      "batch_nr:  87\n",
      "loss:  22.826759338378906\n",
      "batch_nr:  88\n",
      "loss:  16.408966064453125\n",
      "batch_nr:  89\n",
      "loss:  16.725173950195312\n",
      "batch_nr:  90\n",
      "loss:  22.107805252075195\n",
      "batch_nr:  91\n",
      "loss:  15.691065788269043\n",
      "batch_nr:  92\n",
      "loss:  23.569091796875\n",
      "batch_nr:  93\n",
      "loss:  22.783733367919922\n",
      "batch_nr:  94\n",
      "loss:  27.035602569580078\n",
      "batch_nr:  95\n",
      "loss:  24.063987731933594\n",
      "batch_nr:  96\n",
      "loss:  23.575544357299805\n",
      "batch_nr:  97\n",
      "loss:  18.693984985351562\n",
      "batch_nr:  98\n",
      "loss:  24.2857723236084\n",
      "batch_nr:  99\n",
      "loss:  17.64240837097168\n",
      "batch_nr:  100\n",
      "loss:  7.836336135864258\n",
      "batch_nr:  101\n",
      "loss:  11.622302055358887\n",
      "batch_nr:  102\n",
      "loss:  15.742944717407227\n",
      "batch_nr:  103\n",
      "loss:  15.124691009521484\n",
      "batch_nr:  104\n",
      "loss:  19.246965408325195\n",
      "batch_nr:  105\n",
      "loss:  16.671539306640625\n",
      "batch_nr:  106\n",
      "loss:  23.10524559020996\n",
      "batch_nr:  107\n",
      "loss:  26.619239807128906\n",
      "batch_nr:  108\n",
      "loss:  11.497535705566406\n",
      "batch_nr:  109\n",
      "loss:  11.31135082244873\n",
      "batch_nr:  110\n",
      "loss:  20.983009338378906\n",
      "batch_nr:  111\n",
      "loss:  24.4494686126709\n",
      "batch_nr:  112\n",
      "loss:  11.567380905151367\n",
      "batch_nr:  113\n",
      "loss:  25.696819305419922\n",
      "batch_nr:  114\n",
      "loss:  30.827993392944336\n",
      "batch_nr:  115\n",
      "loss:  22.00979232788086\n",
      "batch_nr:  116\n",
      "loss:  17.38414192199707\n",
      "batch_nr:  117\n",
      "loss:  24.693750381469727\n",
      "batch_nr:  118\n",
      "loss:  20.064998626708984\n",
      "batch_nr:  119\n",
      "loss:  23.15218734741211\n",
      "batch_nr:  120\n",
      "loss:  15.37979507446289\n",
      "batch_nr:  121\n",
      "loss:  18.83657455444336\n",
      "batch_nr:  122\n",
      "loss:  25.968074798583984\n",
      "batch_nr:  123\n",
      "loss:  12.938246726989746\n",
      "batch_nr:  124\n",
      "loss:  15.34050178527832\n",
      "batch_nr:  125\n",
      "loss:  18.747037887573242\n",
      "batch_nr:  126\n",
      "loss:  15.327041625976562\n",
      "batch_nr:  127\n",
      "loss:  17.14005470275879\n",
      "batch_nr:  128\n",
      "loss:  8.044689178466797\n",
      "batch_nr:  129\n",
      "loss:  22.7021484375\n",
      "batch_nr:  130\n",
      "loss:  22.66493797302246\n",
      "batch_nr:  131\n",
      "loss:  22.953752517700195\n",
      "batch_nr:  132\n",
      "loss:  14.816746711730957\n",
      "batch_nr:  133\n",
      "loss:  24.229406356811523\n",
      "batch_nr:  134\n",
      "loss:  21.91758155822754\n",
      "batch_nr:  135\n",
      "loss:  20.68806266784668\n",
      "batch_nr:  136\n",
      "loss:  12.575782775878906\n",
      "batch_nr:  137\n",
      "loss:  30.43766975402832\n",
      "batch_nr:  138\n",
      "loss:  17.504817962646484\n",
      "batch_nr:  139\n",
      "loss:  15.807001113891602\n",
      "batch_nr:  140\n",
      "loss:  14.415328979492188\n",
      "batch_nr:  141\n",
      "loss:  20.045207977294922\n",
      "batch_nr:  142\n",
      "loss:  31.478668212890625\n",
      "batch_nr:  143\n",
      "loss:  17.045801162719727\n",
      "batch_nr:  144\n",
      "loss:  25.736114501953125\n",
      "batch_nr:  145\n",
      "loss:  20.394563674926758\n",
      "batch_nr:  146\n",
      "loss:  17.8721981048584\n",
      "batch_nr:  147\n",
      "loss:  20.19162368774414\n",
      "batch_nr:  148\n",
      "loss:  17.436426162719727\n",
      "batch_nr:  149\n",
      "loss:  19.55550765991211\n",
      "batch_nr:  150\n",
      "loss:  23.21202850341797\n",
      "batch_nr:  151\n",
      "loss:  16.56523895263672\n",
      "batch_nr:  152\n",
      "loss:  19.150659561157227\n",
      "batch_nr:  153\n",
      "loss:  24.842191696166992\n",
      "batch_nr:  154\n",
      "loss:  13.596710205078125\n",
      "batch_nr:  155\n",
      "loss:  21.183088302612305\n",
      "batch_nr:  156\n",
      "loss:  19.938737869262695\n",
      "batch_nr:  157\n",
      "loss:  20.70332145690918\n",
      "batch_nr:  158\n",
      "loss:  26.172908782958984\n",
      "batch_nr:  159\n",
      "loss:  23.096590042114258\n",
      "batch_nr:  160\n",
      "loss:  6.96235466003418\n",
      "batch_nr:  161\n",
      "loss:  11.217058181762695\n",
      "batch_nr:  162\n",
      "loss:  15.213356018066406\n",
      "batch_nr:  163\n",
      "loss:  25.349308013916016\n",
      "batch_nr:  164\n",
      "loss:  23.83249855041504\n",
      "batch_nr:  165\n",
      "loss:  29.00314712524414\n",
      "batch_nr:  166\n",
      "loss:  19.443376541137695\n",
      "batch_nr:  167\n",
      "loss:  7.730809688568115\n",
      "batch_nr:  168\n",
      "loss:  13.005864143371582\n",
      "batch_nr:  169\n",
      "loss:  6.676676273345947\n",
      "batch_nr:  170\n",
      "loss:  18.480093002319336\n",
      "batch_nr:  171\n",
      "loss:  12.69648265838623\n",
      "batch_nr:  172\n",
      "loss:  11.904876708984375\n",
      "batch_nr:  173\n",
      "loss:  16.192092895507812\n",
      "batch_nr:  174\n",
      "loss:  16.88392448425293\n",
      "batch_nr:  175\n",
      "loss:  13.601384162902832\n",
      "batch_nr:  176\n",
      "loss:  16.07416534423828\n",
      "batch_nr:  177\n",
      "loss:  14.993456840515137\n",
      "batch_nr:  178\n",
      "loss:  13.697623252868652\n",
      "batch_nr:  179\n",
      "loss:  23.699434280395508\n",
      "batch_nr:  180\n",
      "loss:  34.7967414855957\n",
      "batch_nr:  181\n",
      "loss:  15.922354698181152\n",
      "batch_nr:  182\n",
      "loss:  9.587640762329102\n",
      "batch_nr:  183\n",
      "loss:  26.688108444213867\n",
      "batch_nr:  184\n",
      "loss:  19.96511459350586\n",
      "batch_nr:  185\n",
      "loss:  26.154272079467773\n",
      "batch_nr:  186\n",
      "loss:  24.222061157226562\n",
      "batch_nr:  187\n",
      "loss:  11.366480827331543\n",
      "batch_nr:  188\n",
      "loss:  13.988880157470703\n",
      "batch_nr:  189\n",
      "loss:  16.483598709106445\n",
      "batch_nr:  190\n",
      "loss:  17.39543914794922\n",
      "batch_nr:  191\n",
      "loss:  25.48151969909668\n",
      "batch_nr:  192\n",
      "loss:  24.968708038330078\n",
      "batch_nr:  193\n",
      "loss:  16.18027687072754\n",
      "batch_nr:  194\n",
      "loss:  22.115156173706055\n",
      "batch_nr:  195\n",
      "loss:  30.169157028198242\n",
      "batch_nr:  196\n",
      "loss:  11.1480131149292\n",
      "batch_nr:  197\n",
      "loss:  29.23291015625\n",
      "batch_nr:  198\n",
      "loss:  26.954734802246094\n",
      "batch_nr:  199\n",
      "loss:  21.405088424682617\n",
      "batch_nr:  200\n",
      "loss:  16.234575271606445\n",
      "batch_nr:  201\n",
      "loss:  25.481115341186523\n",
      "batch_nr:  202\n",
      "loss:  13.997725486755371\n",
      "batch_nr:  203\n",
      "loss:  22.728689193725586\n",
      "batch_nr:  204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  18.538537979125977\n",
      "batch_nr:  205\n",
      "loss:  19.599334716796875\n",
      "batch_nr:  206\n",
      "loss:  12.123735427856445\n",
      "batch_nr:  207\n",
      "loss:  10.877176284790039\n",
      "batch_nr:  208\n",
      "loss:  25.34575080871582\n",
      "batch_nr:  209\n",
      "loss:  12.860795021057129\n",
      "batch_nr:  210\n",
      "loss:  20.207881927490234\n",
      "batch_nr:  211\n",
      "loss:  22.900226593017578\n",
      "batch_nr:  212\n",
      "loss:  20.671958923339844\n",
      "batch_nr:  213\n",
      "loss:  26.457462310791016\n",
      "batch_nr:  214\n",
      "loss:  20.19486427307129\n",
      "batch_nr:  215\n",
      "loss:  6.408452987670898\n",
      "batch_nr:  216\n",
      "loss:  17.274702072143555\n",
      "batch_nr:  217\n",
      "loss:  21.10826873779297\n",
      "batch_nr:  218\n",
      "loss:  19.198881149291992\n",
      "batch_nr:  219\n",
      "loss:  18.036794662475586\n",
      "batch_nr:  220\n",
      "loss:  20.364213943481445\n",
      "batch_nr:  221\n",
      "loss:  25.359813690185547\n",
      "batch_nr:  222\n",
      "loss:  13.205558776855469\n",
      "batch_nr:  223\n",
      "loss:  17.053560256958008\n",
      "batch_nr:  224\n",
      "loss:  6.878844261169434\n",
      "batch_nr:  225\n",
      "loss:  12.478123664855957\n",
      "batch_nr:  226\n",
      "loss:  15.829628944396973\n",
      "batch_nr:  227\n",
      "loss:  21.004255294799805\n",
      "batch_nr:  228\n",
      "loss:  20.372575759887695\n",
      "batch_nr:  229\n",
      "loss:  22.855541229248047\n",
      "batch_nr:  230\n",
      "loss:  5.64887809753418\n",
      "batch_nr:  231\n",
      "loss:  23.19257926940918\n",
      "batch_nr:  232\n",
      "loss:  17.913734436035156\n",
      "batch_nr:  233\n",
      "loss:  25.87393569946289\n",
      "batch_nr:  234\n",
      "loss:  21.858562469482422\n",
      "batch_nr:  235\n",
      "loss:  19.17728614807129\n",
      "batch_nr:  236\n",
      "loss:  18.92677879333496\n",
      "batch_nr:  237\n",
      "loss:  16.747997283935547\n",
      "batch_nr:  238\n",
      "loss:  21.80071258544922\n",
      "batch_nr:  239\n",
      "loss:  11.643783569335938\n",
      "batch_nr:  240\n",
      "loss:  14.599004745483398\n",
      "batch_nr:  241\n",
      "loss:  16.216331481933594\n",
      "batch_nr:  242\n",
      "loss:  15.933938980102539\n",
      "batch_nr:  243\n",
      "loss:  25.183046340942383\n",
      "batch_nr:  244\n",
      "loss:  15.35672378540039\n",
      "batch_nr:  245\n",
      "loss:  15.555699348449707\n",
      "batch_nr:  246\n",
      "loss:  15.624584197998047\n",
      "batch_nr:  247\n",
      "loss:  14.215789794921875\n",
      "batch_nr:  248\n",
      "loss:  24.401826858520508\n",
      "batch_nr:  249\n",
      "loss:  26.830793380737305\n",
      "batch_nr:  250\n",
      "loss:  10.484456062316895\n",
      "batch_nr:  251\n",
      "loss:  27.177520751953125\n",
      "batch_nr:  252\n",
      "loss:  18.415706634521484\n",
      "batch_nr:  253\n",
      "loss:  10.397439956665039\n",
      "batch_nr:  254\n",
      "loss:  8.878302574157715\n",
      "batch_nr:  255\n",
      "loss:  16.553537368774414\n",
      "batch_nr:  256\n",
      "loss:  20.900222778320312\n",
      "batch_nr:  257\n",
      "loss:  13.731369972229004\n",
      "batch_nr:  258\n",
      "loss:  14.044432640075684\n",
      "batch_nr:  259\n",
      "loss:  8.326188087463379\n",
      "batch_nr:  260\n",
      "loss:  27.71953010559082\n",
      "batch_nr:  261\n",
      "loss:  11.961260795593262\n",
      "batch_nr:  262\n",
      "loss:  12.067034721374512\n",
      "batch_nr:  263\n",
      "loss:  12.081666946411133\n",
      "batch_nr:  264\n",
      "loss:  21.82750701904297\n",
      "batch_nr:  265\n",
      "loss:  16.796812057495117\n",
      "batch_nr:  266\n",
      "loss:  24.92012596130371\n",
      "batch_nr:  267\n",
      "loss:  19.118988037109375\n",
      "batch_nr:  268\n",
      "loss:  15.172751426696777\n",
      "batch_nr:  269\n",
      "loss:  17.017301559448242\n",
      "batch_nr:  270\n",
      "loss:  6.4082465171813965\n",
      "batch_nr:  271\n",
      "loss:  7.016327857971191\n",
      "batch_nr:  272\n",
      "loss:  29.76291275024414\n",
      "batch_nr:  273\n",
      "loss:  24.438648223876953\n",
      "batch_nr:  274\n",
      "loss:  18.782390594482422\n",
      "batch_nr:  275\n",
      "loss:  21.81068992614746\n",
      "batch_nr:  276\n",
      "loss:  12.151782035827637\n",
      "batch_nr:  277\n",
      "loss:  8.792749404907227\n",
      "batch_nr:  278\n",
      "loss:  10.319430351257324\n",
      "batch_nr:  279\n",
      "loss:  19.014497756958008\n",
      "batch_nr:  280\n",
      "loss:  19.77596092224121\n",
      "batch_nr:  281\n",
      "loss:  24.809003829956055\n",
      "batch_nr:  282\n",
      "loss:  28.52367401123047\n",
      "batch_nr:  283\n",
      "loss:  12.484092712402344\n",
      "batch_nr:  284\n",
      "loss:  18.047496795654297\n",
      "batch_nr:  285\n",
      "loss:  18.844167709350586\n",
      "batch_nr:  286\n",
      "loss:  19.564586639404297\n",
      "batch_nr:  287\n",
      "loss:  20.07099723815918\n",
      "batch_nr:  288\n",
      "loss:  26.82259750366211\n",
      "batch_nr:  289\n",
      "loss:  8.946089744567871\n",
      "batch_nr:  290\n",
      "loss:  16.864315032958984\n",
      "batch_nr:  291\n",
      "loss:  30.306446075439453\n",
      "batch_nr:  292\n",
      "loss:  24.74072265625\n",
      "batch_nr:  293\n",
      "loss:  16.468280792236328\n",
      "batch_nr:  294\n",
      "loss:  13.356330871582031\n",
      "batch_nr:  295\n",
      "loss:  14.74733829498291\n",
      "batch_nr:  296\n",
      "loss:  23.739614486694336\n",
      "batch_nr:  297\n",
      "loss:  20.72928810119629\n",
      "batch_nr:  298\n",
      "loss:  22.975404739379883\n",
      "batch_nr:  299\n",
      "loss:  21.28658676147461\n",
      "batch_nr:  300\n",
      "loss:  24.004560470581055\n",
      "batch_nr:  301\n",
      "loss:  16.318695068359375\n",
      "batch_nr:  302\n",
      "loss:  22.352294921875\n",
      "batch_nr:  303\n",
      "loss:  26.25727653503418\n",
      "batch_nr:  304\n",
      "loss:  8.388068199157715\n",
      "batch_nr:  305\n",
      "loss:  19.03329086303711\n",
      "batch_nr:  306\n",
      "loss:  17.795684814453125\n",
      "batch_nr:  307\n",
      "loss:  14.565839767456055\n",
      "batch_nr:  308\n",
      "loss:  28.985164642333984\n",
      "batch_nr:  309\n",
      "loss:  16.68016815185547\n",
      "batch_nr:  310\n",
      "loss:  15.033611297607422\n",
      "batch_nr:  311\n",
      "loss:  14.023799896240234\n",
      "batch_nr:  312\n",
      "loss:  19.650300979614258\n",
      "batch_nr:  313\n",
      "loss:  34.356201171875\n",
      "batch_nr:  314\n",
      "loss:  29.229228973388672\n",
      "batch_nr:  315\n",
      "loss:  24.57679557800293\n",
      "batch_nr:  316\n",
      "loss:  20.08661651611328\n",
      "batch_nr:  317\n",
      "loss:  30.8431339263916\n",
      "batch_nr:  318\n",
      "loss:  25.979101181030273\n",
      "batch_nr:  319\n",
      "loss:  29.016963958740234\n",
      "batch_nr:  320\n",
      "loss:  16.453197479248047\n",
      "batch_nr:  321\n",
      "loss:  20.43482780456543\n",
      "batch_nr:  322\n",
      "loss:  10.494500160217285\n",
      "batch_nr:  323\n",
      "loss:  27.676599502563477\n",
      "batch_nr:  324\n",
      "loss:  20.37384605407715\n",
      "batch_nr:  325\n",
      "loss:  4.619330883026123\n",
      "batch_nr:  326\n",
      "loss:  18.0739803314209\n",
      "batch_nr:  327\n",
      "loss:  19.472244262695312\n",
      "batch_nr:  328\n",
      "loss:  25.804277420043945\n",
      "batch_nr:  329\n",
      "loss:  25.567020416259766\n",
      "batch_nr:  330\n",
      "loss:  28.18852424621582\n",
      "batch_nr:  331\n",
      "loss:  12.814107894897461\n",
      "batch_nr:  332\n",
      "loss:  9.82816219329834\n",
      "batch_nr:  333\n",
      "loss:  38.06313705444336\n",
      "batch_nr:  334\n",
      "loss:  20.175939559936523\n",
      "batch_nr:  335\n",
      "loss:  15.065136909484863\n",
      "batch_nr:  336\n",
      "loss:  12.001195907592773\n",
      "batch_nr:  337\n",
      "loss:  18.772478103637695\n",
      "batch_nr:  338\n",
      "loss:  27.725812911987305\n",
      "batch_nr:  339\n",
      "loss:  25.245649337768555\n",
      "batch_nr:  340\n",
      "loss:  23.078630447387695\n",
      "batch_nr:  341\n",
      "loss:  14.190852165222168\n",
      "batch_nr:  342\n",
      "loss:  28.504283905029297\n",
      "batch_nr:  343\n",
      "loss:  11.125298500061035\n",
      "batch_nr:  344\n",
      "loss:  26.477415084838867\n",
      "batch_nr:  345\n",
      "loss:  14.726362228393555\n",
      "batch_nr:  346\n",
      "loss:  25.348670959472656\n",
      "batch_nr:  347\n",
      "loss:  16.29637336730957\n",
      "batch_nr:  348\n",
      "loss:  22.208030700683594\n",
      "batch_nr:  349\n",
      "loss:  22.852323532104492\n",
      "batch_nr:  350\n",
      "loss:  12.798829078674316\n",
      "batch_nr:  351\n",
      "loss:  13.824094772338867\n",
      "batch_nr:  352\n",
      "loss:  27.57351303100586\n",
      "batch_nr:  353\n",
      "loss:  15.168728828430176\n",
      "batch_nr:  354\n",
      "loss:  21.047710418701172\n",
      "batch_nr:  355\n",
      "loss:  21.762405395507812\n",
      "batch_nr:  356\n",
      "loss:  7.401209831237793\n",
      "batch_nr:  357\n",
      "loss:  20.553730010986328\n",
      "batch_nr:  358\n",
      "loss:  22.621593475341797\n",
      "batch_nr:  359\n",
      "loss:  20.055988311767578\n",
      "batch_nr:  360\n",
      "loss:  17.485530853271484\n",
      "batch_nr:  361\n",
      "loss:  21.060087203979492\n",
      "batch_nr:  362\n",
      "loss:  18.848499298095703\n",
      "batch_nr:  363\n",
      "loss:  20.702613830566406\n",
      "batch_nr:  364\n",
      "loss:  19.942995071411133\n",
      "batch_nr:  365\n",
      "loss:  21.609987258911133\n",
      "batch_nr:  366\n",
      "loss:  10.21327018737793\n",
      "batch_nr:  367\n",
      "loss:  19.916597366333008\n",
      "batch_nr:  368\n",
      "loss:  12.964736938476562\n",
      "batch_nr:  369\n",
      "loss:  20.885194778442383\n",
      "batch_nr:  370\n",
      "loss:  19.474220275878906\n",
      "batch_nr:  371\n",
      "loss:  20.083980560302734\n",
      "batch_nr:  372\n",
      "loss:  20.808134078979492\n",
      "batch_nr:  373\n",
      "loss:  18.977237701416016\n",
      "batch_nr:  374\n",
      "loss:  17.02045249938965\n",
      "batch_nr:  375\n",
      "loss:  18.298795700073242\n",
      "batch_nr:  376\n",
      "loss:  7.806107044219971\n",
      "batch_nr:  377\n",
      "loss:  20.176267623901367\n",
      "batch_nr:  378\n",
      "loss:  13.637500762939453\n",
      "batch_nr:  379\n",
      "loss:  15.415156364440918\n",
      "batch_nr:  380\n",
      "loss:  15.76052474975586\n",
      "batch_nr:  381\n",
      "loss:  27.078310012817383\n",
      "batch_nr:  382\n",
      "loss:  12.203827857971191\n",
      "batch_nr:  383\n",
      "loss:  15.37929630279541\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1b4512813233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-1b4512813233>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loss: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__convertor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misatty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_and_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_and_convert\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    188\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_ansi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mcursor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_plain_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\colorama\\ansitowin32.py\u001b[0m in \u001b[0;36mwrite_plain_text\u001b[1;34m(self, text, start, end)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mflush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mimport_lock_held\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                 \u001b[0mevt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 347\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    348\u001b[0m                 \u001b[1;31m# and give a timeout to avoid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflush_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel\\iostream.py\u001b[0m in \u001b[0;36mschedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_events\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# wake event thread (message content is ignored)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\zmq\\sugar\\socket.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    393\u001b[0m                                  copy_threshold=self.copy_threshold)\n\u001b[0;32m    394\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSocket\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msend_multipart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg_parts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mzmq\\backend\\cython\\socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "########## Convolutional neural net ############\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "#für GPU\n",
    "#device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./data',download=True,train=True,transform=transforms.Compose\n",
    "                                                          ([transforms.ToTensor()])),batch_size=10,shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data',download=True,train=False,transform=transforms.Compose\n",
    "                                                         ([transforms.ToTensor()])),batch_size=10,shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CNN =nn.Sequential(\n",
    "                   nn.Conv2d(1,32,2),\n",
    "                   nn.ReLU(),\n",
    "                   \n",
    "                   nn.MaxPool2d(2),\n",
    "                   nn.Conv2d(32,128,2),\n",
    "                   nn.ReLU(),\n",
    "                   \n",
    "                   nn.Linear(12,48),\n",
    "                   nn.ReLU(),\n",
    "                   \n",
    "                   nn.Linear(48,96),\n",
    "                   nn.ReLU(),\n",
    "    \n",
    "                   nn.Linear(96,10),\n",
    "                   nn.ReLU()\n",
    "                   )\n",
    "#CNN=CNN.to(device)\n",
    "criterion = torch.nn.MSELoss()            \n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(CNN.parameters(),lr=0.25)\n",
    "def train(epoch):\n",
    "    \n",
    "    \n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        print(\"batch_nr: \",batch_id)\n",
    "        data =torch.Tensor(data)\n",
    "        data=data.float()\n",
    "        #data=data.to(device)\n",
    "        target = torch.Tensor(label.float())\n",
    "        target=target.float()\n",
    "        #target=target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = CNN(data)\n",
    "\n",
    "        loss =criterion(preds, target)\n",
    "        print(\"loss: \",loss.item())\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "train(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
