{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3797da65ef10>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_capability\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36mcurrent_device\u001b[1;34m()\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcurrent_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m     \u001b[1;34mr\"\"\"Returns the index of a currently selected device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m     \u001b[0m_lazy_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    352\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_getDevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    160\u001b[0m         raise RuntimeError(\n\u001b[0;32m    161\u001b[0m             \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[1;32m--> 162\u001b[1;33m     \u001b[0m_check_driver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[0m_cudart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\cuda\\__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[1;34m()\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[0mAlternatively\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgo\u001b[0m \u001b[0mto\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mpytorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morg\u001b[0m \u001b[0mto\u001b[0m \u001b[0minstall\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[0ma\u001b[0m \u001b[0mPyTorch\u001b[0m \u001b[0mversion\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mcompiled\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0myour\u001b[0m \u001b[0mversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m of the CUDA driver.\"\"\".format(str(torch._C._cuda_getDriverVersion())))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nThe NVIDIA driver on your system is too old (found version 9010).\nPlease update your GPU driver by downloading and installing a new\nversion from the URL: http://www.nvidia.com/Download/index.aspx\nAlternatively, go to: https://pytorch.org to install\na PyTorch version that has been compiled with your version\nof the CUDA driver."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_capability(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "print(torch.cuda.get_device_properties(0))\n",
    "torch.cuda.set_device(0)\n",
    "print(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=1, out_features=10, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [1.]], requires_grad=True)\n",
      "Sigmoid()\n",
      "Linear(in_features=10, out_features=1, bias=True)\n",
      "Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)\n",
      "Sequential(\n",
      "  (0): Linear(in_features=1, out_features=10, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n",
      "epoch:  1 Loss:  31.21257781982422\n",
      "epoch:  2 Loss:  9.828660011291504\n",
      "epoch:  3 Loss:  2.1334309577941895\n",
      "epoch:  4 Loss:  0.9510778188705444\n",
      "epoch:  5 Loss:  0.6984660029411316\n",
      "epoch:  6 Loss:  0.6390078067779541\n",
      "epoch:  7 Loss:  0.6100386381149292\n",
      "epoch:  8 Loss:  0.5868361592292786\n",
      "epoch:  9 Loss:  0.5651189088821411\n",
      "epoch:  10 Loss:  0.5441954731941223\n",
      "epoch:  11 Loss:  0.5239441990852356\n",
      "epoch:  12 Loss:  0.5043361783027649\n",
      "epoch:  13 Loss:  0.48535823822021484\n",
      "epoch:  14 Loss:  0.46699798107147217\n",
      "epoch:  15 Loss:  0.4492436647415161\n",
      "epoch:  16 Loss:  0.43208402395248413\n",
      "epoch:  17 Loss:  0.4155070185661316\n",
      "epoch:  18 Loss:  0.3995008170604706\n",
      "epoch:  19 Loss:  0.3840527832508087\n",
      "epoch:  20 Loss:  0.3691505789756775\n",
      "epoch:  21 Loss:  0.35478198528289795\n",
      "epoch:  22 Loss:  0.34093353152275085\n",
      "epoch:  23 Loss:  0.32759326696395874\n",
      "epoch:  24 Loss:  0.3147481083869934\n",
      "epoch:  25 Loss:  0.3023848235607147\n",
      "epoch:  26 Loss:  0.29049059748649597\n",
      "epoch:  27 Loss:  0.2790524661540985\n",
      "epoch:  28 Loss:  0.26805710792541504\n",
      "epoch:  29 Loss:  0.2574918568134308\n",
      "epoch:  30 Loss:  0.2473435401916504\n",
      "epoch:  31 Loss:  0.23759903013706207\n",
      "epoch:  32 Loss:  0.22824633121490479\n",
      "epoch:  33 Loss:  0.2192719429731369\n",
      "epoch:  34 Loss:  0.21066391468048096\n",
      "epoch:  35 Loss:  0.20240961015224457\n",
      "epoch:  36 Loss:  0.19449684023857117\n",
      "epoch:  37 Loss:  0.18691356480121613\n",
      "epoch:  38 Loss:  0.1796479970216751\n",
      "epoch:  39 Loss:  0.17268872261047363\n",
      "epoch:  40 Loss:  0.16602447628974915\n",
      "epoch:  41 Loss:  0.15964385867118835\n",
      "epoch:  42 Loss:  0.15353623032569885\n",
      "epoch:  43 Loss:  0.1476905345916748\n",
      "epoch:  44 Loss:  0.1420968472957611\n",
      "epoch:  45 Loss:  0.13674518465995789\n",
      "epoch:  46 Loss:  0.13162577152252197\n",
      "epoch:  47 Loss:  0.12672877311706543\n",
      "epoch:  48 Loss:  0.12204532325267792\n",
      "epoch:  49 Loss:  0.11756635457277298\n",
      "epoch:  50 Loss:  0.1132831871509552\n",
      "epoch:  51 Loss:  0.10918755829334259\n",
      "epoch:  52 Loss:  0.10527122765779495\n",
      "epoch:  53 Loss:  0.10152672976255417\n",
      "epoch:  54 Loss:  0.09794613718986511\n",
      "epoch:  55 Loss:  0.09452242404222488\n",
      "epoch:  56 Loss:  0.09124848991632462\n",
      "epoch:  57 Loss:  0.08811803162097931\n",
      "epoch:  58 Loss:  0.08512409776449203\n",
      "epoch:  59 Loss:  0.08226054906845093\n",
      "epoch:  60 Loss:  0.07952164858579636\n",
      "epoch:  61 Loss:  0.07690171897411346\n",
      "epoch:  62 Loss:  0.07439520210027695\n",
      "epoch:  63 Loss:  0.07199668139219284\n",
      "epoch:  64 Loss:  0.06970137357711792\n",
      "epoch:  65 Loss:  0.06750427931547165\n",
      "epoch:  66 Loss:  0.0654011070728302\n",
      "epoch:  67 Loss:  0.06338715553283691\n",
      "epoch:  68 Loss:  0.061458393931388855\n",
      "epoch:  69 Loss:  0.059610702097415924\n",
      "epoch:  70 Loss:  0.05784038454294205\n",
      "epoch:  71 Loss:  0.05614394694566727\n",
      "epoch:  72 Loss:  0.0545174814760685\n",
      "epoch:  73 Loss:  0.0529579259455204\n",
      "epoch:  74 Loss:  0.05146213248372078\n",
      "epoch:  75 Loss:  0.05002713203430176\n",
      "epoch:  76 Loss:  0.04864991083741188\n",
      "epoch:  77 Loss:  0.04732801020145416\n",
      "epoch:  78 Loss:  0.0460585281252861\n",
      "epoch:  79 Loss:  0.044839046895504\n",
      "epoch:  80 Loss:  0.04366754740476608\n",
      "epoch:  81 Loss:  0.042541444301605225\n",
      "epoch:  82 Loss:  0.04145865514874458\n",
      "epoch:  83 Loss:  0.040417175740003586\n",
      "epoch:  84 Loss:  0.039415352046489716\n",
      "epoch:  85 Loss:  0.03845079615712166\n",
      "epoch:  86 Loss:  0.037522390484809875\n",
      "epoch:  87 Loss:  0.036628324538469315\n",
      "epoch:  88 Loss:  0.035766854882240295\n",
      "epoch:  89 Loss:  0.034936513751745224\n",
      "epoch:  90 Loss:  0.03413600102066994\n",
      "epoch:  91 Loss:  0.033364005386829376\n",
      "epoch:  92 Loss:  0.03261904418468475\n",
      "epoch:  93 Loss:  0.031900182366371155\n",
      "epoch:  94 Loss:  0.031206119805574417\n",
      "epoch:  95 Loss:  0.030535804107785225\n",
      "epoch:  96 Loss:  0.02988819219172001\n",
      "epoch:  97 Loss:  0.029262255877256393\n",
      "epoch:  98 Loss:  0.028657088056206703\n",
      "epoch:  99 Loss:  0.0280718095600605\n",
      "epoch:  100 Loss:  0.027505574747920036\n",
      "epoch:  101 Loss:  0.026957673951983452\n",
      "epoch:  102 Loss:  0.0264271330088377\n",
      "epoch:  103 Loss:  0.025913501158356667\n",
      "epoch:  104 Loss:  0.025415809825062752\n",
      "epoch:  105 Loss:  0.024933533743023872\n",
      "epoch:  106 Loss:  0.024466099217534065\n",
      "epoch:  107 Loss:  0.024012818932533264\n",
      "epoch:  108 Loss:  0.02357320673763752\n",
      "epoch:  109 Loss:  0.02314668335020542\n",
      "epoch:  110 Loss:  0.022732749581336975\n",
      "epoch:  111 Loss:  0.022330863401293755\n",
      "epoch:  112 Loss:  0.021940600126981735\n",
      "epoch:  113 Loss:  0.021561626344919205\n",
      "epoch:  114 Loss:  0.021193258464336395\n",
      "epoch:  115 Loss:  0.020835280418395996\n",
      "epoch:  116 Loss:  0.020487377420067787\n",
      "epoch:  117 Loss:  0.02014900930225849\n",
      "epoch:  118 Loss:  0.019819946959614754\n",
      "epoch:  119 Loss:  0.019499778747558594\n",
      "epoch:  120 Loss:  0.019188184291124344\n",
      "epoch:  121 Loss:  0.018884917721152306\n",
      "epoch:  122 Loss:  0.018589606508612633\n",
      "epoch:  123 Loss:  0.018302109092473984\n",
      "epoch:  124 Loss:  0.018022025004029274\n",
      "epoch:  125 Loss:  0.017749225720763206\n",
      "epoch:  126 Loss:  0.017483266070485115\n",
      "epoch:  127 Loss:  0.017224090173840523\n",
      "epoch:  128 Loss:  0.01697140373289585\n",
      "epoch:  129 Loss:  0.016725054010748863\n",
      "epoch:  130 Loss:  0.016484679654240608\n",
      "epoch:  131 Loss:  0.016250334680080414\n",
      "epoch:  132 Loss:  0.016021622344851494\n",
      "epoch:  133 Loss:  0.015798388049006462\n",
      "epoch:  134 Loss:  0.015580535866320133\n",
      "epoch:  135 Loss:  0.015367870219051838\n",
      "epoch:  136 Loss:  0.015160243026912212\n",
      "epoch:  137 Loss:  0.014957420527935028\n",
      "epoch:  138 Loss:  0.014759308658540249\n",
      "epoch:  139 Loss:  0.014565812423825264\n",
      "epoch:  140 Loss:  0.014376729726791382\n",
      "epoch:  141 Loss:  0.014191924594342709\n",
      "epoch:  142 Loss:  0.014011314138770103\n",
      "epoch:  143 Loss:  0.013834725134074688\n",
      "epoch:  144 Loss:  0.013662111945450306\n",
      "epoch:  145 Loss:  0.013493291102349758\n",
      "epoch:  146 Loss:  0.01332817506045103\n",
      "epoch:  147 Loss:  0.013166670687496662\n",
      "epoch:  148 Loss:  0.01300865225493908\n",
      "epoch:  149 Loss:  0.012854030355811119\n",
      "epoch:  150 Loss:  0.012702819891273975\n",
      "epoch:  151 Loss:  0.012554711662232876\n",
      "epoch:  152 Loss:  0.012409772723913193\n",
      "epoch:  153 Loss:  0.012267913669347763\n",
      "epoch:  154 Loss:  0.012128930538892746\n",
      "epoch:  155 Loss:  0.011992866173386574\n",
      "epoch:  156 Loss:  0.011859557591378689\n",
      "epoch:  157 Loss:  0.01172911562025547\n",
      "epoch:  158 Loss:  0.011601187288761139\n",
      "epoch:  159 Loss:  0.011475883424282074\n",
      "epoch:  160 Loss:  0.011353176087141037\n",
      "epoch:  161 Loss:  0.011232846416532993\n",
      "epoch:  162 Loss:  0.011114980094134808\n",
      "epoch:  163 Loss:  0.010999390855431557\n",
      "epoch:  164 Loss:  0.010886085219681263\n",
      "epoch:  165 Loss:  0.010775038041174412\n",
      "epoch:  166 Loss:  0.01066608913242817\n",
      "epoch:  167 Loss:  0.01055927388370037\n",
      "epoch:  168 Loss:  0.010454499162733555\n",
      "epoch:  169 Loss:  0.010351736098527908\n",
      "epoch:  170 Loss:  0.010250912047922611\n",
      "epoch:  171 Loss:  0.010152016766369343\n",
      "epoch:  172 Loss:  0.010054980404675007\n",
      "epoch:  173 Loss:  0.009959770366549492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0264], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################  Erstes handmade NN  ####################\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "n_in =10#input\n",
    "n_h=5 #hidden layer size\n",
    "n_out=1#output\n",
    "batch_size=10#batch size\n",
    "y = torch.tensor([[0.],[1.],[2.],[3.],[4.],[5.]])\n",
    "x=torch.tensor([[0.],[1.],[2.],[3.],[4.],[5.]])\n",
    "def init_weights(m):\n",
    "    print(m)\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(1.0)\n",
    "        print(m.weight)\n",
    "net =nn.Sequential(nn.Linear(1,10),\n",
    "                   nn.Sigmoid(),\n",
    "                   nn.Linear(10, 1),\n",
    "                   )\n",
    "net.apply(init_weights)\n",
    "            \n",
    "criterion = torch.nn.MSELoss()#wie wird loss berechnet            \n",
    "loss=criterion(y[1],x[0])       \n",
    "\n",
    "epoch=0\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.1)\n",
    "\n",
    "\n",
    "while loss.item()>0.01:\n",
    "    epoch+=1\n",
    "    y_pred = net(x)#model.predict --> ist das selbe in sklearn\n",
    "\n",
    "    # loss wird anhand von criterion berechnet\n",
    "    loss = criterion(y_pred, y)\n",
    "    print(\"epoch: \",epoch,\"Loss: \",loss.item())\n",
    "\n",
    "    # Gradienten auf null setzen\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backwardpass aller Gewichte und Biases anhand Backpropagation\n",
    "    loss.backward()\n",
    "\n",
    "    # Parameterupdate\n",
    "    optimizer.step()\n",
    "net(x[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:02, 3407708.71it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 84288.49it/s]                                                                                          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 1298673.53it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 31895.40it/s]                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\max\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([10, 128, 26, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########## Convolutional neural net ############\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "from torchvision import datasets,transforms\n",
    "#f√ºr GPU\n",
    "#device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./data',download=True,train=True,transform=transforms.Compose\n",
    "                                                          ([transforms.ToTensor()])),batch_size=10,shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./data',download=True,train=False,transform=transforms.Compose\n",
    "                                                         ([transforms.ToTensor()])),batch_size=10,shuffle=True)\n",
    "\n",
    "class MyNet(nn.Module):\n",
    "    \n",
    "    def __init__():\n",
    "        # init weights or layers\n",
    "    \n",
    "    def forward():\n",
    "        raise NotImplementedError \n",
    "    \n",
    "\n",
    "CNN =nn.Sequential(\n",
    "                   nn.Conv2d(1,32,2),\n",
    "                   nn.ReLU(),\n",
    "                   \n",
    "                   \n",
    "                   nn.Conv2d(32,128,2),\n",
    "                   nn.ReLU(),\n",
    "                   \n",
    "                   nn.Linear(26,52),\n",
    "                   nn.ReLU(),\n",
    "                   \n",
    "                   nn.Linear(52,104),\n",
    "                   nn.ReLU(),\n",
    "    \n",
    "                   nn.Linear(104,10),\n",
    "                   nn.ReLU()\n",
    "                   )\n",
    "#CNN=CNN.to(device)\n",
    "criterion = torch.nn.MSELoss()            \n",
    "def plotter(x,y):\n",
    "    plt.plot(x,y, label=\"loss\")\n",
    "    plt.xlabel(\"iteration*50\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.pause(0.1)\n",
    "    \n",
    "optimizer = torch.optim.Adam(CNN.parameters(),lr=0.25)\n",
    "def train():\n",
    "    last_l=0\n",
    "    last_loss=[]\n",
    "    \n",
    "    q=0\n",
    "    \n",
    "    lossplot=[]\n",
    "    for batch_id, (data, label) in enumerate(train_loader):\n",
    "        last_loss.append(float(last_l))\n",
    "        data =torch.Tensor(data)\n",
    "        data=data.float()\n",
    "        #data=data.to(device)\n",
    "        target = torch.Tensor(label.float())\n",
    "        target=target.float()\n",
    "        #target=target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = CNN(data)\n",
    "        loss =criterion(preds, target)\n",
    "        loss.backward()\n",
    "        last_l=loss.item()\n",
    "        optimizer.step()\n",
    "        q+=1\n",
    "        if q==50:\n",
    "            lossplot.append(np.mean(last_loss))\n",
    "           \n",
    "            q=0\n",
    "            it=np.arange(0,len(lossplot))\n",
    "            plotter(it,lossplot)\n",
    "        \n",
    "        \n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
